% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CVonestep.R
\name{CVonestep_bisec}
\alias{CVonestep_bisec}
\title{Threshold selection based on cross-fit one-step corrected estimator via bisection search}
\usage{
CVonestep_bisec(
  A,
  X,
  Y,
  scores,
  tau.interval = c(0, 1),
  error.bound = 0.05,
  conf.level = 0.95,
  nfolds = 5,
  g.control = list(SL.library = c("SL.glm", "SL.gam", "SL.randomForest")),
  Q.control = list(SL.library = c("SL.glm", "SL.gam", "SL.randomForest")),
  g.trunc = 0.01,
  tol = 0.01
)
}
\arguments{
\item{A}{vector of population indicator. 1 for source population, 0 for target population}

\item{X}{data frame of covariates with each row being one observation}

\item{Y}{vector of dependent variable/outcome. For data from the target population (\code{A=0}), set the corresponding entries of \code{Y} to be \code{NA}}

\item{scores}{either a function assigning scores of \code{Y} given \code{X} trained using an independent dataset from source population or a vector of this function evaluated at observed \verb{(X,Y)}, taking \code{NA} for observations from the target population. If it is a function, it must take input \verb{(x,y)}, where \code{x} is one row of \code{X} (a data frame with one row) and \code{y} is a nonmissing value of \code{Y}, and output a scalar}

\item{tau.interval}{a numeric vector specifying the initial interval of candidate thresholds for bisection search, default to \code{c(0,1)}, which is suitable when all scores lie in the interval \eqn{[0,1]}{[0,1]}}

\item{error.bound}{desired bound on the prediction set coverage error between 0 and 1, default 0.05}

\item{conf.level}{desired level of confidence of low coverage error between 0.5 and 1, default to 0.95}

\item{nfolds}{number of folds for sample splitting, default to 5}

\item{g.control}{a named list containing options passed to \code{\link[SuperLearner:SuperLearner]{SuperLearner::SuperLearner}} to estimate propensity score \code{g}. Must not specify \code{Y}, \code{X}, \code{newX} or \code{family}. Default to \code{list(SL.library=c("SL.glm","SL.gam","SL.randomForest"))}}

\item{Q.control}{a named list containing options passed to \code{\link[SuperLearner:SuperLearner]{SuperLearner::SuperLearner}} to estimate conditional coverage error \code{Q}. Must not specify \code{Y}, \code{X}, \code{newX} or \code{family}. Default to \code{list(SL.library=c("SL.glm","SL.gam","SL.randomForest"))}}

\item{g.trunc}{truncation level of propensity score \code{g} from zero, default to 0.01}

\item{tol}{stopping criterion for bisection search, default to 0.01. The search will stop if the length of the interval for thresholds is below \code{tol} and the function will output the end point with confidence upper bound no greater than \code{error.bound}}
}
\value{
a list with the following components:
\describe{
\item{\code{tau}}{Selected tau}
\item{\code{error.CI.upper}}{The (approximate) confidence upper bound of coverage error corresponding to the select tau}
\item{\code{error.est}}{The point estimate of coverage error corresponding to the selected tau}
}
}
\description{
Method to select a threshold for APAC prediction sets based on cross-fit one-step corrected estimators via bisection serach
}
\section{Warnings/Errors due to extreme candidate thresholds}{

When extremely small/large thresholds are included in \code{candidata.tau}, it is common to receive warnings/errors from the machine learning algorithms used by \code{\link[SuperLearner:SuperLearner]{SuperLearner::SuperLearner}}, because in such cases, almost all \code{Y} are included in (for small thresholds) or excluded from (for large thresholds) the corresponding prediction sets, leading to complaints from machine learning algorithms. This is usually not an issue because the resulting predictions are still quite accurate. We also strongly encourage the user to specify a lerner that can deal with such cases (e.g., \code{SL.glm}) in \code{Q.control}.
}

\examples{
n<-100
expit<-function(x) 1/(1+exp(-x))
A<-rbinom(n,1,.5)
X<-data.frame(X=rnorm(n,sd=ifelse(A==1,1,.5)))
Y<-rbinom(n,1,expit(1+X$X))
scores<-dbinom(Y,1,expit(.08+1.1*X$X))
CVonestep_bisec(A,X,Y,scores,nfolds=2,
                g.control=list(SL.library="SL.glm"),
                Q.control=list(SL.library="SL.glm"))
}
